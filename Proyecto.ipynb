{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final - Big Data (Problema #1)\n",
    "**Estudiante: Justin Arce**\n",
    "\n",
    "Este notebook contiene el pipeline completo (Fases 1-5) para el Problema #1: Predicción de la gravedad de accidentes de bicicleta en Madrid. El notebook está diseñado para ejecutarse de inicio a fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 0: Instalación de Dependencias y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar 'optuna' para la Optimización de Hiperparámetros (HPO)\n",
    "!pip3 install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Importación de Librerías --- \n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, expr, cast, concat_ws, lag, to_date, lit, substring, coalesce, when, regexp_replace, trim, udf, row_number\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, FeatureHasher, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de Spark y Base de Datos (basado en Tarea3_bigdata.ipynb) --- \n",
    "\n",
    "# 1. Iniciar Sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoBigData_SingleNotebook\") \\\n",
    "    .config(\"spark.jars\", \"/src/postgresql-42.2.14.jar\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Rutas de Datos Crudos\n",
    "PATH_ACCIDENTES = \"datos/accidentes/\"\n",
    "PATH_ESTACIONES = \"datos/estaciones/Estaciones_control_datos_meteorologicos.csv\"\n",
    "PATH_METEO = \"datos/meteo/\"\n",
    "\n",
    "# 3. Configuración de Base de Datos PostgreSQL (para Fases 2 y 3)\n",
    "# Usamos la IP del gateway de Docker y el puerto expuesto de 'start_local_env.sh'\n",
    "DB_URL = \"jdbc:postgresql://172.17.0.1:5433/accidentes_db\"\n",
    "DB_TABLE = \"dataset_final_ml\"\n",
    "DB_PROPS = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"testPassword\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"encoding\": \"UTF-8\" \n",
    "}\n",
    "\n",
    "# 4. Semilla Fija para Reproducibilidad\n",
    "SEED = 42\n",
    "\n",
    "print(\"Spark y configuración listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1: ETL - Preprocesamiento de Fuentes\n",
    "\n",
    "**Objetivo:** Cargar, limpiar, transformar y unir (append) las 3 fuentes de datos por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funciones de Ayuda (Sanitización) ---\n",
    "\n",
    "def sanitizar_columnas_string(df: DataFrame, columnas: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia espacios en blanco (trim) y remueve caracteres de nueva línea\n",
    "    en las columnas de string especificadas.\n",
    "    \"\"\"\n",
    "    df_sanitizado = df\n",
    "    for c in columnas:\n",
    "        if c in df.columns:\n",
    "            df_sanitizado = df_sanitizado.withColumn(\n",
    "                c,\n",
    "                trim(regexp_replace(col(c), \"[\\n\\r]\", \"\"))\n",
    "            )\n",
    "    return df_sanitizado\n",
    "\n",
    "# --- Funciones de Fuente A (Accidentes) ---\n",
    "\n",
    "def cargar_datos_fuente_a(spark: SparkSession, ruta_directorio: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Carga y une (append) todos los CSV del directorio 'datos/accidentes/'.\n",
    "    Maneja los schemas diferentes (lesividad vs tipo_lesividad).\n",
    "    Limpia coordenadas y fechas.\n",
    "    \"\"\"\n",
    "    print(f\"Cargando Fuente A (Accidentes) desde {ruta_directorio}...\")\n",
    "\n",
    "    # Cargar todos los CSVs en el directorio.\n",
    "    # No usamos inferSchema, castearemos manualmente para robustez.\n",
    "    df_unido = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \";\") \\\n",
    "        .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "        .csv(ruta_directorio)\n",
    "\n",
    "    # 1. Manejar Schemas Diferentes (lesividad vs tipo_lesividad)\n",
    "    if \"lesividad\" not in df_unido.columns:\n",
    "        df_unido = df_unido.withColumn(\"lesividad\", lit(None).cast(StringType()))\n",
    "    if \"tipo_lesividad\" not in df_unido.columns:\n",
    "        df_unido = df_unido.withColumn(\"tipo_lesividad\", lit(None).cast(StringType()))\n",
    "\n",
    "    df_con_lesividad = df_unido.withColumn(\n",
    "        \"lesividad_unida\",\n",
    "        coalesce(col(\"lesividad\"), col(\"tipo_lesividad\"))\n",
    "    ).drop(\"lesividad\", \"tipo_lesividad\")\n",
    "\n",
    "    # 2. Limpiar Coordenadas (Quitar '.' de miles y reemplazar ',' decimal)\n",
    "    df_coords_limpias = df_con_lesividad.withColumn(\n",
    "        \"coordenada_x_utm_limpia\",\n",
    "        regexp_replace(col(\"coordenada_x_utm\"), \"[\\\\.]\", \"\") # Quitar puntos de miles\n",
    "    ).withColumn(\n",
    "        \"coordenada_x_utm_limpia\",\n",
    "        regexp_replace(col(\"coordenada_x_utm_limpia\"), \",\", \".\") # Reemplazar coma decimal\n",
    "    ).withColumn(\n",
    "        \"coordenada_y_utm_limpia\",\n",
    "        regexp_replace(col(\"coordenada_y_utm\"), \"[\\\\.]\", \"\")\n",
    "    ).withColumn(\n",
    "        \"coordenada_y_utm_limpia\",\n",
    "        regexp_replace(col(\"coordenada_y_utm_limpia\"), \",\", \".\")\n",
    "    ).withColumn(\n",
    "        \"coordenada_x_utm\",\n",
    "        col(\"coordenada_x_utm_limpia\").cast(DoubleType())\n",
    "    ).withColumn(\n",
    "        \"coordenada_y_utm\",\n",
    "        col(\"coordenada_y_utm_limpia\").cast(DoubleType())\n",
    "    ).drop(\"coordenada_x_utm_limpia\", \"coordenada_y_utm_limpia\")\n",
    "    \n",
    "    # 3. Limpiar y Formatear Fecha y Hora\n",
    "    df_fecha_limpia = df_coords_limpias.withColumn(\n",
    "        \"fecha\",\n",
    "        to_date(col(\"fecha\"), \"dd/MM/yyyy\")\n",
    "    ).withColumn(\n",
    "        \"hora\",\n",
    "        concat_ws(\"H\", lit(\"\"), substring(col(\"hora\"), 1, 2)) # \"10:00:00\" -> \"H10\"\n",
    "    )\n",
    "    \n",
    "    # 4. Sanitizar columnas de string\n",
    "    columnas_string = [\"tipo_accidente\", \"distrito\", \"sexo\", \"estado_meteorológico\"]\n",
    "    df_sanitizado = sanitizar_columnas_string(df_fecha_limpia, columnas_string)\n",
    "\n",
    "    return df_sanitizado\n",
    "\n",
    "def crear_variable_objetivo(df_accidentes: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Crea la variable objetivo binaria 'accidente_grave'.\n",
    "    \"\"\"\n",
    "    print(\"Creando variable objetivo 'accidente_grave'...\")\n",
    "    \n",
    "    df_con_lesividad_int = df_accidentes.withColumn(\n",
    "        \"cod_lesividad_int\",\n",
    "        col(\"cod_lesividad\").cast(IntegerType())\n",
    "    )\n",
    "\n",
    "    df_con_objetivo = df_con_lesividad_int.withColumn(\n",
    "        \"accidente_grave\",\n",
    "        when(\n",
    "            (col(\"cod_lesividad_int\") == 3) | (col(\"cod_lesividad_int\") == 4), 1\n",
    "        ).otherwise(0)\n",
    "    ).drop(\"cod_lesividad_int\")\n",
    "    \n",
    "    return df_con_objetivo\n",
    "\n",
    "# --- Funciones de Fuente B (Estaciones) ---\n",
    "\n",
    "def cargar_datos_fuente_b(spark: SparkSession, ruta_archivo: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Carga el CSV de Estaciones (Fuente B).\n",
    "    \"\"\"\n",
    "    print(f\"Cargando Fuente B (Estaciones) desde {ruta_archivo}...\")\n",
    "    \n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \";\") \\\n",
    "        .option(\"encoding\", \"ISO-8859-1\") # Corrección para 'CÓDIGO'\n",
    "        .csv(ruta_archivo)\n",
    "        \n",
    "    df_limpio = df.withColumn(\n",
    "        \"COORDENADA_X_ETRS89\",\n",
    "        regexp_replace(col(\"COORDENADA_X_ETRS89\"), \",\", \".\").cast(DoubleType())\n",
    "    ).withColumn(\n",
    "        \"COORDENADA_Y_ETRS89\",\n",
    "        regexp_replace(col(\"COORDENADA_Y_ETRS89\"), \",\", \".\").cast(DoubleType())\n",
    "    )\n",
    "    \n",
    "    df_seleccionado = df_limpio.select(\n",
    "        col(\"CÓDIGO\").alias(\"COD_ESTACION_B\"),\n",
    "        col(\"CÓDIGO_CORTO\").cast(IntegerType()),\n",
    "        col(\"COORDENADA_X_ETRS89\"),\n",
    "        col(\"COORDENADA_Y_ETRS89\")\n",
    "    )\n",
    "    \n",
    "    return df_seleccionado\n",
    "\n",
    "# --- Funciones de Fuente C (Meteo) ---\n",
    "\n",
    "def procesar_fuente_c(spark: SparkSession, ruta_directorio: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Carga todos los CSVs de 'datos/meteo/', los une, y aplica los dos pivotes\n",
    "    y la ingeniería de features temporales (lag).\n",
    "    \"\"\"\n",
    "    print(f\"Procesando Fuente C (Meteo) desde {ruta_directorio}...\")\n",
    "\n",
    "    # Cargar TODOS los CSVs en el directorio de meteo\n",
    "    df_raw = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \";\") \\\n",
    "        .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "        .csv(ruta_directorio)\n",
    "\n",
    "    # Forzar columnas H y V a String ANTES del stack (para evitar error DATATYPE_MISMATCH)\n",
    "    df_casteado = df_raw\n",
    "    h_cols_v_cols = [c for c in df_raw.columns if c.startswith('H') or c.startswith('V')]\n",
    "    for c_name in h_cols_v_cols:\n",
    "        df_casteado = df_casteado.withColumn(c_name, col(c_name).cast(StringType()))\n",
    "\n",
    "    # 1. Primer Pivote (Stack)\n",
    "    stack_expr_list = []\n",
    "    for i in range(1, 25):\n",
    "        h_col = f\"H{i:02d}\"\n",
    "        v_col = f\"V{i:02d}\"\n",
    "        # Corrección: stack espera (constante, valor, constante, valor...)\n",
    "        stack_expr_list.append(f\"'{h_col}', {h_col}, {v_col}\")\n",
    "    \n",
    "    stack_expr = f\"stack(24, {', '.join(stack_expr_list)}) AS (HORA, VALOR, VALIDACION)\"\n",
    "    \n",
    "    df_largo = df_casteado.select(\n",
    "        col(\"ESTACION\").cast(IntegerType()),\n",
    "        col(\"MAGNITUD\").cast(IntegerType()),\n",
    "        col(\"ANO\").cast(IntegerType()),\n",
    "        col(\"MES\").cast(IntegerType()),\n",
    "        col(\"DIA\").cast(IntegerType()),\n",
    "        expr(stack_expr)\n",
    "    )\n",
    "\n",
    "    # Filtrar solo valores válidos ('V') y limpiar comas\n",
    "    df_valido = df_largo.filter(trim(col(\"VALIDACION\")) == \"V\") \\\n",
    "                         .withColumn(\"VALOR_NUM\", \n",
    "                                     regexp_replace(col(\"VALOR\"), \",\", \".\").cast(DoubleType())) \\\n",
    "                         .drop(\"VALOR\", \"VALIDACION\")\n",
    "\n",
    "    # 2. Segundo Pivote (Magnitudes)\n",
    "    df_pivotado = df_valido.groupBy(\"ESTACION\", \"ANO\", \"MES\", \"DIA\", \"HORA\") \\\n",
    "                           .pivot(\"MAGNITUD\", [81, 83, 89]) \\\n",
    "                           .avg(\"VALOR_NUM\") \\\n",
    "                           .withColumnRenamed(\"81\", \"VV_81_t=0\") \\\n",
    "                           .withColumnRenamed(\"83\", \"T_83_t=0\") \\\n",
    "                           .withColumnRenamed(\"89\", \"P_89_t=0\")\n",
    "\n",
    "    # 3. Ingeniería de Features Temporales (Lag)\n",
    "    window_spec = Window.partitionBy(\"ESTACION\").orderBy(\"ANO\", \"MES\", \"DIA\", \"HORA\")\n",
    "\n",
    "    df_con_lag = df_pivotado.withColumn(\"T_83_t-1h\", lag(\"T_83_t=0\", 1).over(window_spec)) \\\n",
    "                            .withColumn(\"VV_81_t-1h\", lag(\"VV_81_t=0\", 1).over(window_spec)) \\\n",
    "                            .withColumn(\"P_89_t-1h\", lag(\"P_89_t=0\", 1).over(window_spec)) \\\n",
    "                            .withColumn(\"T_83_t-2h\", lag(\"T_83_t=0\", 2).over(window_spec)) \\\n",
    "                            .withColumn(\"VV_81_t-2h\", lag(\"VV_81_t=0\", 2).over(window_spec)) \\\n",
    "                            .withColumn(\"P_89_t-2h\", lag(\"P_89_t=0\", 2).over(window_spec))\n",
    "\n",
    "    # 4. Formatear Fecha y renombrar\n",
    "    df_final_meteo = df_con_lag.withColumn(\n",
    "        \"FECHA\",\n",
    "        to_date(concat_ws(\"-\", col(\"ANO\"), col(\"MES\"), col(\"DIA\")), \"yyyy-MM-dd\")\n",
    "    ).withColumnRenamed(\"ESTACION\", \"COD_ESTACION\")\n",
    "\n",
    "    # Rellenar nulos (de lags y pivots) con 0\n",
    "    df_relleno = df_final_meteo.na.fill(0)\n",
    "    \n",
    "    return df_relleno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EJECUTAR FASE 1 --- \n",
    "start_time = time.time()\n",
    "\n",
    "print(\"--- INICIANDO FASE 1: PREPROCESAMIENTO ---\")\n",
    "\n",
    "df_a_cargado = cargar_datos_fuente_a(spark, PATH_ACCIDENTES)\n",
    "df_accidentes = crear_variable_objetivo(df_a_cargado)\n",
    "df_estaciones = cargar_datos_fuente_b(spark, PATH_ESTACIONES)\n",
    "df_meteo = procesar_fuente_c(spark, PATH_METEO)\n",
    "\n",
    "df_accidentes.cache()\n",
    "df_estaciones.cache()\n",
    "df_meteo.cache()\n",
    "\n",
    "print(f\"\\n--- FASE 1 COMPLETADA en {time.time() - start_time:.2f} segundos ---\")\n",
    "print(f\"Filas Fuente A (Accidentes): {df_accidentes.count()}\")\n",
    "print(f\"Filas Fuente B (Estaciones): {df_estaciones.count()}\")\n",
    "print(f\"Filas Fuente C (Meteo Procesado): {df_meteo.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2: Cruce de Fuentes y Materialización\n",
    "\n",
    "**Objetivo:** Aplicar el cruce espacial (A+B) y temporal (ABC) y guardar el resultado en PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funciones de Fase 2 (Cruce y Materialización) ---\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def calcular_distancia_euclidiana(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    UDF para calcular la distancia euclidiana.\n",
    "    \"\"\"\n",
    "    if x1 is None or y1 is None or x2 is None or y2 is None:\n",
    "        return float('inf')\n",
    "    return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "def cruzar_fuentes(spark: SparkSession, df_accidentes: DataFrame, df_estaciones: DataFrame, df_meteo: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Implementa la lógica de cruce espacial (A+B) y temporal (Fase 2).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Iniciando Fase 2.1: Cruce Espacial (A+B)...\")\n",
    "    # 1. Cruce Espacial (A + B)\n",
    "    df_distancias = df_accidentes.crossJoin(df_estaciones)\n",
    "    \n",
    "    df_con_dist = df_distancias.withColumn(\n",
    "        \"distancia\",\n",
    "        calcular_distancia_euclidiana(\n",
    "            col(\"coordenada_x_utm\"), col(\"coordenada_y_utm\"),\n",
    "            col(\"COORDENADA_X_ETRS89\"), col(\"COORDENADA_Y_ETRS89\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Seleccionar la estación más cercana (rank=1)\n",
    "    window_dist = Window.partitionBy(\"num_expediente\").orderBy(\"distancia\")\n",
    "    \n",
    "    df_acc_con_estacion = df_con_dist.withColumn(\"rank\", row_number().over(window_dist)) \\\n",
    "                                     .filter(col(\"rank\") == 1) \\\n",
    "                                     .drop(\"rank\", \"distancia\")\n",
    "    \n",
    "    count_pre_temporal = df_acc_con_estacion.count()\n",
    "    print(f\"Cruce espacial completado. Filas resultantes: {count_pre_temporal}\")\n",
    "\n",
    "    # 2. Cruce Temporal (Final)\n",
    "    print(\"Iniciando Fase 2.2: Cruce Temporal (Final)...\")\n",
    "    \n",
    "    df_join = df_acc_con_estacion.join(\n",
    "        df_meteo,\n",
    "        [\n",
    "            df_acc_con_estacion.fecha == df_meteo.FECHA,\n",
    "            df_acc_con_estacion.hora == df_meteo.HORA,\n",
    "            df_acc_con_estacion.CÓDIGO_CORTO == df_meteo.COD_ESTACION\n",
    "        ],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Eliminamos las columnas de la derecha (Meteo) que causan ambigüedad\n",
    "    df_final_ml = df_join \\\n",
    "        .drop(df_meteo.FECHA) \\\n",
    "        .drop(df_meteo.HORA) \\\n",
    "        .drop(df_meteo.COD_ESTACION)\n",
    "    \n",
    "    count_post_temporal = df_final_ml.count()\n",
    "    print(f\"Cruce temporal completado. Filas finales: {count_post_temporal}\")\n",
    "    \n",
    "    # 3. Schema Final\n",
    "    columnas_numericas = [\n",
    "        \"T_83_t=0\", \"VV_81_t=0\", \"P_89_t=0\",\n",
    "        \"T_83_t-1h\", \"VV_81_t-1h\", \"P_89_t-1h\",\n",
    "        \"T_83_t-2h\", \"VV_81_t-2h\", \"P_89_t-2h\"\n",
    "    ]\n",
    "    columnas_categoricas = [\"tipo_accidente\", \"distrito\", \"sexo\", \"estado_meteorológico\"]\n",
    "    columnas_clave = [\"num_expediente\", \"accidente_grave\"]\n",
    "    \n",
    "    columnas_finales = columnas_clave.copy()\n",
    "    \n",
    "    for c in columnas_numericas:\n",
    "        if c in df_final_ml.columns:\n",
    "            columnas_finales.append(c)\n",
    "            \n",
    "    for c in columnas_categoricas:\n",
    "         if c in df_final_ml.columns:\n",
    "            columnas_finales.append(c)\n",
    "            \n",
    "    # Rellenar nulos\n",
    "    df_final_seleccionado = df_final_ml.select(columnas_finales)\n",
    "    df_final_relleno = df_final_seleccionado.na.fill(0, subset=[c for c in columnas_numericas if c in columnas_finales])\n",
    "    df_final_relleno = df_final_relleno.na.fill(\"Desconocido\", subset=[c for c in columnas_categoricas if c in columnas_finales])\n",
    "\n",
    "    return df_final_relleno\n",
    "\n",
    "def materializar_datos(df: DataFrame, url: str, tabla: str, props: dict):\n",
    "    \"\"\"\n",
    "    Escribe el DataFrame final en PostgreSQL.\n",
    "    \"\"\"\n",
    "    print(f\"Materializando en PostgreSQL local (tabla: {tabla})...\")\n",
    "\n",
    "    df.write \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"url\", url) \\\n",
    "      .option(\"dbtable\", tabla) \\\n",
    "      .option(\"user\", props[\"user\"]) \\\n",
    "      .option(\"password\", props[\"password\"]) \\\n",
    "      .option(\"driver\", props[\"driver\"]) \\\n",
    "      .option(\"encoding\", props[\"encoding\"]) \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "    \n",
    "    print(f\"Datos escritos en PostgreSQL, tabla '{tabla}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EJECUTAR FASE 2 --- \n",
    "start_time = time.time()\n",
    "print(\"\\n--- INICIANDO FASE 2: CRUCE Y MATERIALIZACIÓN ---\")\n",
    "\n",
    "df_final_ml = cruzar_fuentes(spark, df_accidentes, df_estaciones, df_meteo)\n",
    "\n",
    "df_final_ml.cache()\n",
    "print(f\"Dataset final de ML generado con {df_final_ml.count()} filas.\")\n",
    "print(\"Schema final:\")\n",
    "df_final_ml.printSchema()\n",
    "\n",
    "print(\"Mostrando 5 filas de muestra (con encoding correcto):\")\n",
    "df_final_ml.show(5, truncate=False)\n",
    "\n",
    "materializar_datos(df_final_ml, DB_URL, DB_TABLE, DB_PROPS)\n",
    "\n",
    "# Limpiar DataFrames cacheados de Fase 1\n",
    "df_accidentes.unpersist()\n",
    "df_estaciones.unpersist()\n",
    "df_meteo.unpersist()\n",
    "\n",
    "print(f\"\\n--- FASE 2 COMPLETADA en {time.time() - start_time:.2f} segundos ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3: EDA (Pre-Experimento)\n",
    "\n",
    "**Objetivo:** Analizar el dataset limpio (`df_final_ml`) antes de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EJECUTAR FASE 3 --- \n",
    "print(\"\\n--- INICIANDO FASE 3: ANÁLISIS EXPLORATORIO DE DATOS (EDA) ---\")\n",
    "\n",
    "# 1. Análisis de Variable Objetivo (sensibilidad a clases)\n",
    "print(\"Análisis de Desbalanceo:\")\n",
    "df_ml.groupBy('accidente_grave').count().show()\n",
    "\n",
    "# 2. Análisis de Predictores Numéricos (histogramas, boxplots, matriz)\n",
    "numeric_cols_to_plot = [c for c in df_ml.columns if c.startswith(('T_', 'VV_', 'P_'))]\n",
    "df_sample_pd = df_ml.select(['accidente_grave'] + numeric_cols_to_plot).sample(fraction=0.1, seed=SEED).toPandas()\n",
    "\n",
    "if not df_sample_pd.empty and numeric_cols_to_plot:\n",
    "    # Histogramas\n",
    "    df_sample_pd[numeric_cols_to_plot].hist(bins=30, figsize=(15, 5))\n",
    "    plt.suptitle(\"Histogramas de Predictores Numéricos\")\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplots Bivariados\n",
    "    sns.boxplot(data=df_sample_pd, x='accidente_grave', y=numeric_cols_to_plot[0])\n",
    "    plt.title(f\"{numeric_cols_to_plot[0]} vs. Gravedad del Accidente\")\n",
    "    plt.show()\n",
    "\n",
    "    # Matriz de Correlación\n",
    "    corr_matrix = df_sample_pd[numeric_cols_to_plot].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title(\"Matriz de Correlación de Features Numéricas\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se encontraron columnas numéricas (T_, VV_, P_) o la muestra está vacía.\")\n",
    "\n",
    "# 3. Dividir Datos (Split) para Experimento\n",
    "training_data, test_data = df_ml.randomSplit([0.8, 0.2], seed=SEED)\n",
    "training_data.cache()\n",
    "test_data.cache()\n",
    "print(f\"Datos divididos: {training_data.count()} para entrenamiento, {test_data.count()} para prueba final.\")\n",
    "\n",
    "print(f\"\\n--- FASE 3 COMPLETADA ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4: Experimento Robusto (DOE)\n",
    "\n",
    "**Objetivo:** Ejecutar el plan \"Robusto Ultra-Eficiente\" (18 corridas) y guardar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funciones de Fase 4 (Experimento) ---\n",
    "\n",
    "def construir_pipeline(modelo, factor_b, factor_c, factor_d):\n",
    "    \"\"\"\n",
    "    Construye dinámicamente el pipeline de Spark ML basado en los factores del DOE.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Factor D: Ventana Temporal\n",
    "    numeric_features = [\"T_83_t=0\", \"VV_81_t=0\", \"P_89_t=0\"] # L1\n",
    "    if factor_d in [\"t-1h\", \"t-2h\"]:\n",
    "        numeric_features += [\"T_83_t-1h\", \"VV_81_t-1h\", \"P_89_t-1h\"] # L2\n",
    "    if factor_d == \"t-2h\":\n",
    "        numeric_features += [\"T_83_t-2h\", \"VV_81_t-2h\", \"P_89_t-2h\"] # L3\n",
    "        \n",
    "    categorical_features = [\"tipo_accidente\", \"distrito\", \"sexo\", \"estado_meteorológico\"]\n",
    "\n",
    "    stages = []\n",
    "    \n",
    "    # Convertir target 'accidente_grave' (Int) a 'label' (Double) para Spark ML\n",
    "    label_converter = col(\"accidente_grave\").cast(DoubleType()).alias(\"label\")\n",
    "    # NOTA: Esto no es una etapa de pipeline, se aplica al DF antes del .fit()\n",
    "\n",
    "    # Factor C: Estrategia Categórica\n",
    "    feature_cols = list(numeric_features)\n",
    "    \n",
    "    if factor_c == \"Drop\":\n",
    "        pass # No se añaden features categóricas\n",
    "    elif factor_c == \"OHE\":\n",
    "        indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in categorical_features]\n",
    "        encoders = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_vec\") for c in categorical_features]\n",
    "        stages += indexers + encoders\n",
    "        feature_cols += [f\"{c}_vec\" for c in categorical_features]\n",
    "    elif factor_c == \"Hasher\":\n",
    "        hasher = FeatureHasher(inputCols=categorical_features, outputCol=\"hashed_features\")\n",
    "        stages.append(hasher)\n",
    "        feature_cols.append(\"hashed_features\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_unscaled\", handleInvalid=\"skip\")\n",
    "    stages.append(assembler)\n",
    "\n",
    "    # Lógica Acoplada (Normalización)\n",
    "    input_features_col = \"features_unscaled\"\n",
    "    if modelo == \"LR\":\n",
    "        scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features_scaled\")\n",
    "        stages.append(scaler)\n",
    "        input_features_col = \"features_scaled\"\n",
    "    \n",
    "    # Modelo (Factor A)\n",
    "    model_instance = None\n",
    "    if modelo == \"LR\":\n",
    "        model_instance = LogisticRegression(featuresCol=input_features_col, labelCol=\"label\")\n",
    "    elif modelo == \"GBT\":\n",
    "        model_instance = GBTClassifier(featuresCol=input_features_col, labelCol=\"label\", seed=SEED)\n",
    "    \n",
    "    stages.append(model_instance)\n",
    "    return Pipeline(stages=stages), model_instance, label_converter\n",
    "\n",
    "def ejecutar_hpo(spark, training_data_with_label, pipeline, model, modelo_str):\n",
    "    \"\"\"\n",
    "    Etapa A: HPO Ultra-Ligera (3 trials, K=3)\n",
    "    \"\"\"\n",
    "    evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderPR\", labelCol=\"label\")\n",
    "\n",
    "    def objective(trial):\n",
    "        param_map = {}\n",
    "        if modelo_str == \"LR\":\n",
    "            regParam = trial.suggest_loguniform(\"regParam\", 1e-4, 1.0)\n",
    "            elasticNetParam = trial.suggest_uniform(\"elasticNetParam\", 0.0, 1.0)\n",
    "            param_map[model.regParam] = regParam\n",
    "            param_map[model.elasticNetParam] = elasticNetParam\n",
    "        elif modelo_str == \"GBT\":\n",
    "            maxDepth = trial.suggest_int(\"maxDepth\", 2, 5)\n",
    "            maxIter = trial.suggest_int(\"maxIter\", 10, 20)\n",
    "            param_map[model.maxDepth] = maxDepth\n",
    "            param_map[model.maxIter] = maxIter\n",
    "        \n",
    "        cv = CrossValidator(\n",
    "            estimator=pipeline,\n",
    "            estimatorParamMaps=[param_map],\n",
    "            evaluator=evaluator,\n",
    "            numFolds=3, # K-Fold=3\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        cv_model = cv.fit(training_data_with_label)\n",
    "        return cv_model.avgMetrics[0] # Maximizar AUC-PR\n",

    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=3) # 3 trials\n",
    "    return study.best_params\n",
    "\n",
    "def medir_robustez(spark, training_data_with_label, pipeline, best_params):\n",
    "    \"\"\"\n",
    "    Etapa B: Medición S/N (K=3, Seed Fija)\n",
    "    \"\"\"\n",
    "    evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderPR\", labelCol=\"label\")\n",
    "    \n",
    "    model_stage = pipeline.getStages()[-1]\n",
    "    param_grid = ParamGridBuilder()\n",
    "    \n",
    "    for param, value in best_params.items():\n",
    "        model_param = model_stage.getParam(param)\n",
    "        param_grid = param_grid.addGrid(model_param, [value])\n",
    "        \n",
    "    param_map = param_grid.build()\n",
    "\n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=param_map,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3, # K-Fold=3\n",
    "        seed=SEED # Semilla Fija\n",
    "    )\n",
    "    \n",
    "    cv_model = cv.fit(training_data_with_label)\n",
    "    \n",
    "    metrics = cv_model.avgMetrics # Esta es la lista de métricas por fold\n",
    "    \n",
    "    metrics_array = np.array(metrics)\n",
    "    metrics_array[metrics_array <= 0] = 1e-9 # Evitar división por cero\n",
    "    \n",
    "    sn_ratio = -10 * np.log10(np.mean(1 / (metrics_array**2)))\n",
    "    \n",
    "    return sn_ratio, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EJECUTAR FASE 4 --- \n",
    "start_time = time.time()\n",
    "print(\"\\n--- INICIANDO FASE 4: EJECUCIÓN DEL EXPERIMENTO DOE ---\")\n",
    "\n",
    "# Definir los 2 Torneos L9 (3^3)\n",
    "MODELOS = [\"LR\", \"GBT\"]\n",
    "DESBALANCEO = [\"Weights\", \"RUS\", \"None\"]\n",
    "CATEGORICA = [\"Drop\", \"OHE\", \"Hasher\"]\n",
    "TEMPORAL = [\"t=0\", \"t-1h\", \"t-2h\"]\n",
    "\n",
    "# Array Ortogonal L9 (3^3) - para factores B, C, D\n",
    "L9 = [\n",
    "  (0, 0, 0), # Run 1 (L1, L1, L1)\n",
    "  (0, 1, 1), # Run 2 (L1, L2, L2)\n",
    "  (0, 2, 2), # Run 3 (L1, L3, L3)\n",
    "  (1, 0, 1), # Run 4 (L2, L1, L2)\n",
    "  (1, 1, 2), # Run 5 (L2, L2, L3)\n",
    "  (1, 2, 0), # Run 6 (L2, L3, L1)\n",
    "  (2, 0, 2), # Run 7 (L3, L1, L3)\n",
    "  (2, 1, 0), # Run 8 (L3, L2, L1)\n",
    "  (2, 2, 1)  # Run 9 (L3, L3, L2)\n",
    "]\n",
    "\n",
    "resultados_doe = []\n",
    "\n",
    "# Iterar sobre los dos modelos (Torneo A y B)\n",
    "for MODELO in MODELOS:\n",
    "    print(f\"\\n--- INICIANDO TORNEO PARA MODELO: {MODELO} ---\")\n",
    "    \n",
    "    # Iterar sobre las 9 corridas L9\n",
    "    for i, (b_idx, c_idx, d_idx) in enumerate(L9):\n",
    "        run_id = i + 1\n",
    "        B_VAL = DESBALANCEO[b_idx]\n",
    "        C_VAL = CATEGORICA[c_idx]\n",
    "        D_VAL = TEMPORAL[d_idx]\n",
    "\n",
    "        print(f\"\\n==> Ejecutando Run {run_id}/9 para {MODELO}: B={B_VAL}, C={C_VAL}, D={D_VAL} <==\")\n",
    "        \n",
    "        # 1. Construir pipeline base\n",
    "        pipeline, model_instance, label_converter = construir_pipeline(MODELO, B_VAL, C_VAL, D_VAL)\n",
    "        \n",
    "        # 2. Aplicar conversión de label a los datos de training\n",
    "        training_data_run = training_data.select(\"*\", label_converter)\n",
    "        \n",
    "        # 3. Etapa A: HPO Ultra-Ligera (3 trials * 3 folds)\n",
    "        print(\"--- Etapa A: HPO ---\")\n",
    "        best_params = ejecutar_hpo(spark, training_data_run, pipeline, model_instance, MODELO)\n",
    "        print(f\"Best Params: {best_params}\")\n",
    "\n",
    "        # 4. Etapa B: Medición S/N (3 folds)\n",
    "        print(\"--- Etapa B: S/N ---\")\n",
    "        pipeline_sn, _, _ = construir_pipeline(MODELO, B_VAL, C_VAL, D_VAL)\n",
    "        sn_ratio, fold_metrics = medir_robustez(spark, training_data_run, pipeline_sn, best_params)\n",
    "        print(f\"S/N Ratio: {sn_ratio:.4f}\")\n",
    "\n",
    "        # 5. Guardar resultados\n",
    "        resultados_doe.append({\n",
    "            \"modelo\": MODELO,\n",
    "            \"run_id\": run_id,\n",
    "            \"desbalanceo\": B_VAL,\n",
    "            \"categorica\": C_VAL,\n",
    "            \"temporal\": D_VAL,\n",
    "            \"sn_ratio\": sn_ratio,\n",
    "            \"best_params\": best_params\n",
    "        })\n",
    "        \n",
    "        print(f\"==> Run {run_id}/9 para {MODELO} completado.\")\n",
    "\n",
    "print(f\"\\n--- FASE 4 COMPLETADA en {time.time() - start_time:.2f} segundos ---\")\n",
    "\n",
    "# Convertir resultados a Pandas DataFrame para análisis\n",
    "df_resultados = pd.DataFrame(resultados_doe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 5: Análisis de Resultados y Duelo Final\n",
    "\n",
    "**Objetivo:** Analizar los S/N Ratios, seleccionar los dos campeones y enfrentarlos en el `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EJECUTAR FASE 5 --- \n",
    "print(\"\\n--- INICIANDO FASE 5: ANÁLISIS Y DUELO FINAL ---\")\n",
    "\n",
    "# 1. Análisis Torneo A (LR)\n",
    "print(\"\\n--- Resultados Torneo LR ---\")\n",
    "df_lr = df_resultados[df_resultados[\"modelo\"] == \"LR\"]\n",
    "if not df_lr.empty:\n",
    "    print(df_lr.sort_values(\"sn_ratio\", ascending=False).to_markdown(index=False))\n",
    "    \n",
    "    # Gráfico de Efectos Principales para LR\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    sns.pointplot(data=df_lr, x=\"desbalanceo\", y=\"sn_ratio\", ax=axes[0], order=DESBALANCEO).set_title(\"Factor B (Desbalanceo) - LR\")\n",
    "    sns.pointplot(data=df_lr, x=\"categorica\", y=\"sn_ratio\", ax=axes[1], order=CATEGORICA).set_title(\"Factor C (Categórica) - LR\")\n",
    "    sns.pointplot(data=df_lr, x=\"temporal\", y=\"sn_ratio\", ax=axes[2], order=TEMPORAL).set_title(\"Factor D (Temporal) - LR\")\n",
    "    plt.suptitle(\"Gráficos de Efectos Principales (S/N Ratio) para Logistic Regression\", fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar Campeón A\n",
    "    campeon_a_run = df_lr.loc[df_lr['sn_ratio'].idxmax()]\n",
    "    print(f\"\\nCampeón Robusto A (LR) (Run ID: {campeon_a_run['run_id']}):\\n{campeon_a_run}\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados para el Torneo LR.\")\n",
    "\n",
    "# 2. Análisis Torneo B (GBT)\n",
    "print(\"\\n--- Resultados Torneo GBT ---\")\n",
    "df_gbt = df_resultados[df_resultados[\"modelo\"] == \"GBT\"]\n",
    "if not df_gbt.empty:\n",
    "    print(df_gbt.sort_values(\"sn_ratio\", ascending=False).to_markdown(index=False))\n",
    "    \n",
    "    # Gráfico de Efectos Principales para GBT\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    sns.pointplot(data=df_gbt, x=\"desbalanceo\", y=\"sn_ratio\", ax=axes[0], order=DESBALANCEO).set_title(\"Factor B (Desbalanceo) - GBT\")\n",
    "    sns.pointplot(data=df_gbt, x=\"categorica\", y=\"sn_ratio\", ax=axes[1], order=CATEGORICA).set_title(\"Factor C (Categórica) - GBT\")\n",
    "    sns.pointplot(data=df_gbt, x=\"temporal\", y=\"sn_ratio\", ax=axes[2], order=TEMPORAL).set_title(\"Factor D (Temporal) - GBT\")\n",
    "    plt.suptitle(\"Gráficos de Efectos Principales (S/N Ratio) para GBT Classifier\", fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar Campeón B\n",
    "    campeon_b_run = df_gbt.loc[df_gbt['sn_ratio'].idxmax()]\n",
    "    print(f\"\\nCampeón Robusto B (GBT) (Run ID: {campeon_b_run['run_id']}):\\n{campeon_b_run}\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados para el Torneo GT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Duelo Final --- \n",
    "print(\"\\n--- Duelo Final en Test Data ---\")\n",
    "\n",
    "# Preparar datos (aplicar conversión de label)\n",
    "_, _, label_converter = construir_pipeline(\"LR\", \"None\", \"Drop\", \"t=0\") # Solo para obtener el 'label_converter'\n",
    "training_data_final = training_data.select(\"*\", label_converter)\n",
    "test_data_final = test_data.select(\"*\", label_converter)\n",
    "\n",
    "# Re-construir y Entrenar Campeón A (LR)\n",
    "if 'campeon_a_run' in locals():\n",
    "    pipeline_a, _ = construir_pipeline(\n",
    "        'LR', \n",
    "        campeon_a_run[\"desbalanceo\"],\n",
    "        campeon_a_run[\"categorica\"],\n",
    "        campeon_a_run[\"temporal\"]\n",
    "    )\n",
    "    pipeline_a.getStages()[-1].setParams(**campeon_a_run[\"best_params\"])\n",
    "    \n",
    "    print(\"Entrenando modelo final A (LR)...\")\n",
    "    modelo_final_a = pipeline_a.fit(training_data_final)\n",
    "    predicciones_a = modelo_final_a.transform(test_data_final)\n",
    "else:\n",
    "    print(\"Campeón A no definido. Saltando duelo.\")\n",
    "\n",
    "# Re-construir y Entrenar Campeón B (GBT)\n",
    "if 'campeon_b_run' in locals():\n",
    "    pipeline_b, _ = construir_pipeline(\n",
    "        'GBT', \n",
    "        campeon_b_run[\"desbalanceo\"],\n",
    "        campeon_b_run[\"categorica\"],\n",
    "        campeon_b_run[\"temporal\"]\n",
    "    )\n",
    "    pipeline_b.getStages()[-1].setParams(**campeon_b_run[\"best_params\"])\n",
    "    \n",
    "    print(\"Entrenando modelo final B (GBT)...\")\n",
    "    modelo_final_b = pipeline_b.fit(training_data_final)\n",
    "    predicciones_b = modelo_final_b.transform(test_data_final)\n",
    "else:\n",
    "    print(\"Campeón B no definido. Saltando duelo.\")\n",
    "\n",
    "# 4. Evaluación Final (AUC-PR y Recall)\n",
    "evaluator_pr = BinaryClassificationEvaluator(metricName=\"areaUnderPR\", labelCol=\"label\")\n",
    "evaluator_recall1 = MulticlassClassificationEvaluator(metricName=\"recallByLabel\", metricLabel=1.0, labelCol=\"label\")\n",
    "\n",
    "if 'predicciones_a' in locals():\n",
    "    auc_pr_a = evaluator_pr.evaluate(predicciones_a)\n",
    "    recall_a = evaluator_recall1.evaluate(predicciones_a)\n",
    "    print(f\"\\nModelo A (LR) - AUC-PR Final: {auc_pr_a:.4f} | Recall (Clase 1): {recall_a:.4f}\")\n",
    "\n",
    "if 'predicciones_b' in locals():\n",
    "    auc_pr_b = evaluator_pr.evaluate(predicciones_b)\n",
    "    recall_b = evaluator_recall1.evaluate(predicciones_b)\n",
    "    print(f\"Modelo B (GBT) - AUC-PR Final: {auc_pr_b:.4f} | Recall (Clase 1): {recall_b:.4f}\")\n",
    "\n",
    "# 5. Matriz de Confusión del Ganador\n",
    "ganador_preds = None\n",
    "ganador_nombre = \"N/A\"\n",
    "\n",
    "if 'auc_pr_b' in locals() and auc_pr_b > auc_pr_a:\n",
    "    ganador_preds = predicciones_b\n",
    "    ganador_nombre = \"GBT Classifier\"\n",
    "elif 'auc_pr_a' in locals():\n",
    "    ganador_preds = predicciones_a\n",
    "    ganador_nombre = \"Logistic Regression\"\n",
    "\n",
    "if ganador_preds:\n",
    "    preds_and_labels = ganador_preds.select(\"prediction\", \"label\").rdd.map(lambda r: (float(r.prediction), float(r.label)))\n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    \n",
    "    print(f\"\\nMatriz de Confusión ({ganador_nombre}) en Test Data:\")\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "else:\n",
    "    print(\"No se pudo determinar un modelo ganador para la matriz de confusión.\")\n",
    "\n",
    "# 6. Conclusión Final\n",
    "print(\"\\n--- Conclusión Final ---\")\n",
    "if 'auc_pr_a' not in locals() and 'auc_pr_b' not in locals():\n",
    "    print(\"No se completaron las ejecuciones para determinar un ganador.\")\n",
    "elif auc_pr_b > auc_pr_a:\n",
    "    print(\"El modelo GBT Classifier es el ganador.\")\n",
    "    print(f\"Justificación: Demostró la arquitectura más robusta (S/N Ratio: {campeon_b_run['sn_ratio']:.2f}) en el DOE \\n y logró el mejor rendimiento (AUC-PR: {auc_pr_b:.4f}) en el Test Data final.\")\n",
    "else:\n",
    "    print(\"El modelo Logistic Regression es el ganador.\")\n",
    "    print(f\"Justificación: Demostró la arquitectura más robusta (S/N Ratio: {campeon_a_run['sn_ratio']:.2f}) en el DOE \\n y logró el mejor rendimiento (AUC-PR: {auc_pr_a:.4f}) en el Test Data final.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}